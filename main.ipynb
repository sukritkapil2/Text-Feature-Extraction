{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, for an input to a machine learning model, we need numerical data. For example, if we have a categorical input, we apply one-hot encoding and use that vector as an input to the model. The question is how can we convert something like text which is at a very high level and has a meaning to it, into a format that can be understood by machines in the form of numbers and vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the traditional methods to extract features from text are based on creating these features in a manual way, through an understanding of the domain. For example, we can use bag of words, TF-IDF representations and other such techniques to get a representation of the text. But more recently, an interesting and powerful technique that has come up, is the learning of features through dimensionality reducation using deep learning neural networks, which learn the features automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to go through some of these methods to understand how it works, so that we can decide upon which technique we want to use in our project and also to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by going through some of the surface traditional methods that involve some manual domain knowledge effort to create features for the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method assumes, that we have a document and a corpus (a lot of documents). In our example and the upcoming examples, we are going to assume that a document is a simple English sentence and hence the corpus would be a list of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sentence we can generate a list of tokens, that will represent the sentence. The important part is the generation of these tokens and then maintaining a count, to represent the text. We will be using Python language throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 important steps involved in this whole process:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming\n",
    "3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming means to trim a word in a way, that it retains its meaning. For example the words leaf and leaves will both be stemmed to leaf and leav respectively.\n",
    "\n",
    "The next step is of lemmatization, that connects the stemmed words to their root words. For example, leaf and leav, both words will be lemmatized to leaf.\n",
    "\n",
    "Hence, we can say that after stemming and lemmatization the words 'leaf' and 'leaves' appearing in a sentence would both be transformed into the word leaf. Let us see how we can do these steps in Python.\n",
    "\n",
    "If you get errors using ntlk, run ntlk.download() in a separate command line to download all the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'we',\n",
       " 'will',\n",
       " 'use',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " '.',\n",
       " 'Let',\n",
       " 'us',\n",
       " 'see',\n",
       " ',',\n",
       " 'how',\n",
       " 'it',\n",
       " 'performs',\n",
       " '.',\n",
       " 'We',\n",
       " 'will',\n",
       " 'use',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'the',\n",
       " 'next',\n",
       " 'time',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = 'Hi! This is a test sentence that we will use to perform tokenization, stemming and lemmatization. Let us see, how it performs. We will use a lot of sentences the next time.'\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the output, we have a lot of words like punctuation marks, and frequent non-useful words like 'the', 'it' etc. All these words are called as stop words, which just help to fill in the sentence but do not provide any meaning or semantics to it.\n",
    "\n",
    "We can get rid of these stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'test',\n",
       " 'sentence',\n",
       " 'use',\n",
       " 'perform',\n",
       " 'tokenization',\n",
       " 'stemming',\n",
       " 'lemmatization',\n",
       " 'let',\n",
       " 'us',\n",
       " 'see',\n",
       " 'performs',\n",
       " 'use',\n",
       " 'lot',\n",
       " 'sentences',\n",
       " 'next',\n",
       " 'time']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def get_filtered_tokens(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_word_tokens = []\n",
    "\n",
    "    # filter out word tokens and punctuations\n",
    "    for token in word_tokens:\n",
    "        if token.lower() not in stop_words and token.lower() not in string.punctuation and token.isalpha():\n",
    "            filtered_word_tokens.append(token.lower())\n",
    "\n",
    "    return filtered_word_tokens\n",
    "\n",
    "get_filtered_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that the tokens are all meaningful words, with all the stop words and punctuations removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed with the stemming and lemmatization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def stem_lmtz_word(word):\n",
    "    return lmtzr.lemmatize(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function by passing few words like 'programming' and 'programmer' and see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "programm\n",
      "program\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "words = ['programming', 'programmer', 'program', 'programs']\n",
    "\n",
    "for word in words:\n",
    "    print(stem_lmtz_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps, we need to get a dataset for ourselves. We will get a semantic analysis dataset. We are using the Apple Twitter Sentiment dataset from data.world. We will only focus on the text column of the dataset currently, to generate tokens and perform other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #AAPL:The 10 best Steve Jobs emails ever...htt...\n",
       "1    RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...\n",
       "2    My cat only chews @apple cords. Such an #Apple...\n",
       "3    I agree with @jimcramer that the #IndividualIn...\n",
       "4         Nobody expects the Spanish Inquisition #AAPL\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Datasets/Apple-Twitter-Sentiment-DFE.csv', encoding='cp1252')\n",
    "data['text'] = data['text'].apply(lambda x: x.strip())\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aapl', 'best', 'steve', 'jobs', 'emails', 'ever', 'http']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_filtered_tokens(data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step that we want to do, is to create a vocabulary. By vocabulary, we mean to create a set of all the words occurring in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "tokens_data = []\n",
    "\n",
    "for index, sentence in data['text'].items():\n",
    "    tokens = []\n",
    "    for token in get_filtered_tokens(sentence):\n",
    "        final_token = stem_lmtz_word(token)\n",
    "        vocab.add(final_token)\n",
    "        tokens.append(final_token)\n",
    "    tokens_data.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are all set. We have the vocabulary ready and all the sentences have been represented with a list of tokens that have stop words removed, stemmed and lemmatization done.\n",
    "\n",
    "The next step is to create a bag of words for each sentence. We need to create a vector representation for each sentence, in which each value will be equal to the frequency of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new vector\n",
    "token_count_dict = {}\n",
    "for token in vocab:\n",
    "    token_count_dict[token] = 0\n",
    "\n",
    "bag_of_words_data = []\n",
    "\n",
    "for sent in tokens_data:\n",
    "    sent_rep = []\n",
    "\n",
    "    # set values\n",
    "    for token in sent:\n",
    "        token_count_dict[token] += 1\n",
    "\n",
    "    for key in token_count_dict:\n",
    "        sent_rep.append(token_count_dict[key])\n",
    "        token_count_dict[key] = 0\n",
    "\n",
    "    bag_of_words_data.append(sent_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF Model\n",
    "\n",
    "Although the bag of words model, does give a way to represent a sentence in a numerical way, it still does not capture any semantics. We now look at tf-idf model that captures the weights of the words in a better way. It provides a way, to give weights to words according to their importance.\n",
    "\n",
    "An important point to be noted here is that, where in bag of words, more weight is given to a word occuring more frequently, it does not really mean that the word is more important. Generally words that occur more frequently in a document can also be insignificant.\n",
    "\n",
    "Hence the tf-idf model, takes into account not only the frequency of a word in a document, it also considers the number of documents the word is present in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{TF} = \\dfrac{\\left(\\text{Word frequency in document}\\right)}{\\left(\\text{Total number of terms in document}\\right)}$\n",
    "\n",
    "$\\text{IDF} = \\log{\\left(\\dfrac{\\text{N}}{1 + \\text{n}}\\right)}\\text{, where}$\n",
    "\n",
    "$\\text{N is total number of documents}$\n",
    "\n",
    "$\\text{n is number of documents that contain the word}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tf data can be calculated from bag of words model\n",
    "\n",
    "curr_index = 0\n",
    "tf_data = []\n",
    "\n",
    "for rep in bag_of_words_data:\n",
    "    num_terms = len(tokens_data[curr_index])\n",
    "    npa = np.asarray(rep, dtype=np.float64)\n",
    "    npa = npa / num_terms\n",
    "    curr_index += 1\n",
    "    tf_data.append(npa.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the idf data, we need to have a dictionary that contains for each token, the number of documents it is present in.\n",
    "\n",
    "We have the token_count_dict that contains all the tokens with a value of zero. We can iterate over all the tokens and set the values as the number of documents in this dictionary.\n",
    "\n",
    "The idf array will have a size equal to the number of tokens and is independent of the current document. So, we can calculate it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "token_count_dict = {}\n",
    "for token in vocab:\n",
    "    token_count_dict[token] = 0\n",
    "\n",
    "idf_data = []\n",
    "\n",
    "# set dict values as number of documents it is present in\n",
    "for sent in tokens_data:\n",
    "    doc_vocab = set()\n",
    "    for token in sent:\n",
    "        doc_vocab.add(token)\n",
    "    for token in doc_vocab:\n",
    "        token_count_dict[token] += 1\n",
    "\n",
    "# total number of documents\n",
    "N = data['text'].shape[0]\n",
    "\n",
    "for key in token_count_dict:\n",
    "    init_val = token_count_dict[key]\n",
    "    final_val = math.log(N/(1+init_val))\n",
    "    token_count_dict[key] = final_val\n",
    "\n",
    "    idf_data.append(final_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have both the tf data for all the sentences and we have the idf data for all the words. We can calculate the tf-idf representations of all sentences, by simply multiplying their tf data with the idf array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = []\n",
    "\n",
    "for tf_rep in tf_data:\n",
    "    tf_npa = np.asarray(tf_rep, dtype=np.float64)\n",
    "    tf_idf_npa = np.multiply(tf_rep, idf_data)\n",
    "    tf_idf_data.append(tf_idf_npa.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520b3cd7e0d030d667d66361117068cd0a0b0010cfefb5bab788fdf0445a2ea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
